---
title: "Misspellings Exploration"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}

library(ggplot2)
library(dplyr)
library(tidyr)
library(tibble)
library(hunspell)
library(stringr)
library(monkeylearn)
library(frequency)
library(tidyverse)

redundant_data <- readRDS("project_data/full_redundant_data.rds")

```



The main point of this exploration is to look at misspellings: the error rate and number of misspellings per page and per document. In order to do this, the Hunspell library will be used to detect any words that are misspelled. This involves using the prepared redundant_data RDS file to extract words individually and run them through the hunspell_check() function. Unfortunately, the error rate can not be accurately predicted using solely this method, and this will be discussed later. However, we can still get a general idea of how misspellings are distributed.

The function below will take in a filtered list of words that are in the document or page, and give a data frame that contains the desired data. This includes the probability of a misspelling (the error rate) and the number of misses within the document. Obviously, the most important statistic is the error rate (due to varying document size) but the number of misspellings is a good additional statistic to have.
```{r}

#Input: a cleaned list of words within the document (or page)
#output: A data frame containing the error rate and the number of misses
df_create <- function(cleaned_list) {
  
  df <- data.frame(0, 0, 0)
  names(df) <- c("doc_num", "prob", "num_miss")
  
  i <- 1
  
  #Loop through the list of cleaned_words
  #List can be representative of words per document or words per page
  for (doc in cleaned_words) {
    doc <- cleaned_words[[i]]
    
    num <- length(doc)
    
    correct <- hunspell_check(doc)
    
    num2 <- length(doc[!correct])
    
    #Add new row to dataframe containing the data
    de <- data.frame(i, (num2 / num), num2)
    names(de) <- c("doc_num", "prob", "num_miss")
    df <- rbind(df, de)
    
    i <- i + 1
    
  }
  
  df <- df[-c(1),]
  
  #Round the probabilities to the second decimal place for easy distribution readability
  df <-
    df %>% filter(is.na(prob) != TRUE) %>% mutate(prob = round(prob, digits = 2))
  
  df
  
}

```


The first thing that will be analyzed is the error rate and number of misspellings per document. This is the most important variable that we would like to extract from the data.

The redundant_data$text variable is a double list that contains the document on the outside index and each page on the inside index. Within each page is a string of text that contains the words extracted from the document. In the two loops below, each document and string within each document is looped through.

As mentioned before, Hunspell in our case is not completely accurate. Hunspell checks the spelling of words based off of a pre-defined dictionary. This does not include ALL words and lastnames. Due to the nature of the proposals being analyzed, there are many complex words, lastnames, and acronyms that are not within Hunspell's built-in dictionary. There are two possible solutions that I have come up with that, if in a more dire situation, could be used to improve the analysis:

  1. Download two dictionaries with one containing all of Facebook last names and the other containing terms surrounding the subject of each proposal (Whitelist)
  2. Using NLP, count the average proportion of last names and subject terms not detected by Hunspell and use that as an error rate to get a more accurate reading.
  
Instead of doing the above two (due to limited resources), any special characters acronyms were filtered out of each string. These strings were then broken up to "tokenize" each word and eventually combined together. The end result is a double list containing each document and all of that documents misspellings detected by Hunspell.
```{r}

cleaned_words <- list()

options(scipen = 10)

i <- 1

for (doc in redundant_data$text) {
  doc_words <- list()
  
  #Loop through each page in the doc
  for (str in doc) {
    
    #Filter out false detections by Hunspell
    words_test <- sapply(str, tolower)
    words_test <- str_replace_all(words_test, "[^a-zA-Z]", " ")
    words_test <- str_trim(words_test)
    words_test <- str_squish(words_test)
    words_test <- strsplit(words_test, " ")
    words_test <- unlist(words_test)
    
    doc_words <- append(doc_words, words_test)
    
    doc_words <- unlist(doc_words, use.names = FALSE)
    
  }
  
  cleaned_words[[i]] <- doc_words
  
  i <- i + 1
  
}

#Create dataframe of calculated data using our misspellings

per_doc <- df_create(cleaned_words)

```


Below are two plots containing a histogram and density plot representation of the distrubtion of per document error rates. The median is highlighted using the dotted blue line, showing that the error rate distribution is slightly skewed to the left of it. However, from these graphs we can see that most documents have an error rate between 0 and 10%. There are data points that fall completely outside of the outlier region and these will be looked at later in the document. 
```{r}

#Histogram
per_doc %>% ggplot(aes(x=prob)) + geom_histogram(bins = 100) +  ylab("Count") + xlab("Error Rate") + ggtitle("Per Doc Error Rate Histogram")

#Density plot
error_dist <- per_doc %>% ggplot(aes(x=prob, y = ..count..), alpha = 0.25) + geom_density(fill = "blue", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(prob)),
            color="blue", linetype="dashed", size=1) +  ylab("Count") + xlab("Error Rate") + ggtitle("Per Doc Error Rate Distribution") + scale_y_continuous(label = function(x) format(x/100))

error_dist

```

Although not as important, the distribution of misspelled words within a document is graphed below. As before, a density plot is created for the number of misspelled words per document. This data looked more skewed in comparison to the previous one, so a logarithmic transformation was done on the data to get achieve normality. In a different setting, we would be able to use that normality to look deeper into variance and other statistics, but it is unneccessary for this project and simply for show.
```{r}

#Density plot
dist <- per_doc %>% ggplot(aes(x=num_miss, y = ..count..)) + geom_density(fill = "#4271AE", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(num_miss)),
            color="blue", linetype="dashed", size=1)  +  ylab("Count") + xlab("Number of Misspelled Words") + ggtitle("Per Doc Distribution of Number of Misspelled Words")

#Transformed density plot
transformed_dist <- per_doc  %>% mutate(transformed_num_miss = sign(num_miss) * log2(abs(num_miss) + 1)) %>% ggplot(aes(x=transformed_num_miss, y = ..count..)) + geom_density(fill = "red", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(transformed_num_miss)),
            color="blue", linetype="dashed", size=1) +  ylab("Count") + xlab("Tranformed Number of Misspelled Words") + ggtitle("Per Doc Transformed Distribution of Number of Misspelled Words")


dist
transformed_dist


```


The method below creates a dataframe for the error rate and number of misspellings for each page. Therefore, it is similar to the one above except only contains one for loop to loop through each page in the entire dataset.
```{r}

#Unlisted in order to get one list containing all pages
words <- unlist(redundant_data$text, use.names = FALSE)

cleaned_words <- list()

options(scipen = 10)

i <- 1

#Looping through each page
for (str in words) {
  words_test <- sapply(str, tolower)
    words_test <- str_replace_all(words_test, "[^a-zA-Z]", " ")
  words_test <- str_trim(words_test)
  words_test <- str_squish(words_test)
  words_test <- strsplit(words_test, " ")
  words_test <- unlist(words_test)
  
  cleaned_words[[i]] <- words_test
  
  i <- i + 1
  
}

df <- df_create(cleaned_words)


```

The two plots below show a histogram and a density plot modeling the distrubtion of different error rates per page. Based on the dotted blue median, the per page error rate seems more skewed than the per document distribution. It also seems to have a wider range of outliers, which will be looked at later in the document
```{r}

df %>% ggplot(aes(x=prob)) + geom_histogram(bins = 100) +  ylab("Count") + xlab("Error Rate") + ggtitle("Per Page Error Rate Histogram")

error_dist <- df %>% ggplot(aes(x=prob, y = ..count..), alpha = 0.25) + geom_density(fill = "blue", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(prob)),
            color="blue", linetype="dashed", size=1) +  ylab("Count") + xlab("Error Rate") + ggtitle("Per Page Error Rate Distribution") + scale_y_continuous(label = function(x) format(x/100))

error_dist


```


Similar to before, two plots (density and a transformed version) were created to show the per page distribution of misspelled words. The density plot was found to be very skewed (similar to the per doc number of misspelled words plot) and therefore a logorithmic transformation was applied. The transformed density plot had some variation in the lower quartile, but follows a normal trend thereafter. As mentioned before, a logarithmic transformation can help normalize data for better modeling. In this project it is not as important so it is mostly for show.
```{r}
dist <- df %>% ggplot(aes(x=num_miss, y = ..count..)) + geom_density(fill = "#4271AE", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(num_miss)),
            color="blue", linetype="dashed", size=1)  +  ylab("Count") + xlab("Number of Misspelled Words") + ggtitle("Per Page Distribution of Number of Misspelled Words")

transformed_dist <- df  %>% mutate(transformed_num_miss = sign(num_miss) * log2(abs(num_miss) + 1)) %>% ggplot(aes(x=transformed_num_miss, y = ..count..)) + geom_density(fill = "red", color = "black", alpha = 0.1) + geom_vline(aes(xintercept=mean(transformed_num_miss)),
            color="blue", linetype="dashed", size=1) +  ylab("Count") + xlab("Tranformed Number of Misspelled Words") + ggtitle("Per Page Transformed Distribution of Number of Misspelled Words")


dist
transformed_dist


```

```{r}

#Takes in a data frame and creates a graph that outlines standard deviation intervals
sd_plot_func <- function(dff, type, plot_type){

sds_to_plot <- seq(-6,6)
sd_df <- dff %>%
  summarize(mean_type= mean(type), sd_type = sd(type)) %>%
  slice(rep_along(sds_to_plot, 1)) %>%
  mutate(sd_to_plot=sds_to_plot) %>%
  mutate(sd_val = mean_type + sd_to_plot * sd_type)



#Plots standard deviation indicators on the distribution
 dff %>%
  ggplot(aes(x=type, y = ..count..)) + 
    geom_density(fill = "#4271AE", color = "black", alpha = 0.1) +
    geom_vline(aes(xintercept=mean(type)), col="blue", size=1) +
    geom_vline(aes(xintercept = sd_val), data=sd_df,
               linetype=2, size=1 - abs(seq(-1,1, len=13))) + ggtitle(plot_type)

}

#Computes a 95% confidence interval for given data
conf_interval_func <- function(df, type){
  
  a <- mean(type)
  s <- sd(type)
  n <- nrow(df)
  e <- qnorm(0.975)*(s/sqrt(n))
  left <- (a - e)
  right <- (a + e)
  
  l <- c(left, right)
  
  l
  
}

```


The four graphs below model the standard deviations for each type of variable that has been calculated thus far. This allows us to have a better visualization of how the data is distributed.

Based on what is shown below, the distributions look to be fairly skewed. Each variable, based on what can be seen visually, tends to have a distribution with a small LQR and a large upper quartile range. For example, the per document number error rate has a range of about 2 standard deviations from the mean in the lower quartile, but has a range of approximately 4 standard deviations in the upper quartile range. 
```{r}


#Per document error rate
sd_plot_func(per_doc, per_doc$prob, "Per Document Error Rate - SD")

#Per Page error rate
sd_plot_func(df, df$prob, "Per Page Error Rate - SD")

#Per document number of misspellings
sd_plot_func(per_doc, per_doc$num_miss, "Per Document Num Mistakes - SD" )

#Per Page number of misspellings
sd_plot_func(df, df$num_miss, "Per Page Num Mistakes - SD")

```


Calculating a 95% confidence interval for the four variables further exposed that there could be a problem with the current method of identifying misspelled words. As seen below, the interval for the per doc error rate is between 6.06% and 6.21%. An error rate this high for important proposals indicates that there are most likely many false positives. To get a better idea of what words could be triggering a false positive, looking at some of the outliers is necessary. 
```{r}
conf_interval_df <- data.frame("", "", 0, 0)
  names(conf_interval_df) <- c("type", "measurement", "lower", "upper")

#confidence interval per doc error rate

de <- data.frame("per doc", "error rate", conf_interval_func(per_doc, per_doc$prob)[1], conf_interval_func(per_doc, per_doc$prob)[2])
names(de) <- c("type", "measurement", "lower", "upper")
conf_interval_df <- rbind(conf_interval_df, de)

#confidence interval per page error rate

de <- data.frame("per page", "error rate", conf_interval_func(df, df$prob)[1], conf_interval_func(df, df$prob)[2])
names(de) <- c("type", "measurement", "lower", "upper")
conf_interval_df <- rbind(conf_interval_df, de)

#confidence interval per doc num mistakes

de <- data.frame("per doc", "num mistakes", conf_interval_func(per_doc, per_doc$num_miss)[1], conf_interval_func(per_doc, per_doc$num_miss)[2])
names(de) <- c("type", "measurement", "lower", "upper")
conf_interval_df <- rbind(conf_interval_df, de)

#confidence interval per page num mistakes

de <- data.frame("per page", "num mistakes", conf_interval_func(df, df$num_miss)[1], conf_interval_func(df, df$num_miss)[2])
names(de) <- c("type", "measurement", "lower", "upper")
conf_interval_df <- rbind(conf_interval_df, de)
conf_interval_df <- conf_interval_df[-c(1),]


conf_interval_df


```

To get a better image of what the outliers would look like on the distribution, below are four graphs that each contain an upper outlier range. The range is within the two red lines.
```{r}


outlier_plot_func <- function(df, type, plot_type){
  
  outlier_df <- df %>%
  summarize(q1=quantile(type, 1/4), q3=quantile(type, 3/4), iqr=IQR(type)) %>%
  slice(rep(1, 2)) %>%
  mutate(multiplier = c(1.5, 3)) %>%
  mutate(lower_outlier_limit = q1 - multiplier * iqr) %>%
  mutate(upper_outlier_limit = q3 + multiplier * iqr)

df %>%
  ggplot(aes(x=type, y = ..count..)) +
    geom_density(fill = "red", color = "black", alpha = 0.1) +
   
    geom_vline(aes(xintercept=upper_outlier_limit), data=outlier_df, color="red")  + ggtitle(plot_type)
  
}

#Per document error rate
outlier_plot_func(per_doc, per_doc$prob, "Per Document Error Rate - Outliers")

#Per Page error rate
outlier_plot_func(df, df$prob, "Per Page Error Rate - Outliers")

#Per document error rate
outlier_plot_func(per_doc, per_doc$num_miss, "Per Document Num Mistakes - Outliers")

#Per Page error rate
outlier_plot_func(df, df$num_miss, "Per Page Num Mistakes - Outliers")

```

Using a similar method as above, the upper_outlier_limit was calculated and we want to take a look at some of the data that falls outside of the upper_outlier_limit. This will give a good idea of what words Hunspell is flagging as a misspelling and we will compare these to documents with a low error rate. Because our most important variable is the per document error rate, we will be looking at extreme points in that data frame only.

Out of the 6 random data points chosen from the document error dataframe, doc_num 1681 had the highest error rate (0.35), so therefore we will be looking soley at that one in order to identify the problem with our current Hunspell method.
```{r}


#Calculate the upper outlier limit

calc_uol <- function(df, type){
  
  outlier_df <- df %>%
  summarize(q1=quantile(type, 1/4), q3=quantile(type, 3/4), iqr=IQR(type)) %>%
  slice(rep(1, 2)) %>%
  mutate(multiplier = c(1.5, 3)) %>%
  mutate(lower_outlier_limit = q1 - multiplier * iqr) %>%
  mutate(upper_outlier_limit = q3 + multiplier * iqr)
  
  uol <- outlier_df$upper_outlier_limit
  
  uol[2]
  
  
}


#Outside outlier limit dfs

#per doc error rate
head(per_doc %>% filter(prob > calc_uol(per_doc, per_doc$prob) ))


#doc position with a high error rate.
per_doc[1681,]



```


By using almost the same method to filter extract misspelled words, the types of words that are being identified as misspellings with Hunspell can be identified. The types of words that stand out are last and first names, states, and other various words that most likely need a larger dictionary in order to be recognized by Hunspell. Also detectd were random strings that seemed to be found as a result of special characters/potential errors with the OCR. These, at least for now, are fine. However, names, states, and other large words are true false positives that throw off the accuracy of our findings.

As mentioned, in order to improve this we would simply need larger dictionaries that whitelist specifically those types of words.
```{r}



high_rate <- redundant_data[1681, ]

clean_find <- function(tested_data){
  
  cleaned_words <- list()

i <- 1


for (str in tested_data$text) {
  doc_words <- list()
  
  #Loop through each page in the doc
  
  #Filter out false detections by Hunspell
  words_test <- sapply(str, tolower)
  words_test <- str_replace_all(words_test, "[^a-zA-Z]", " ")
 
  words_test <- str_trim(words_test)
  words_test <- str_squish(words_test)
  words_test <- strsplit(words_test, " ")
  words_test <- unlist(words_test)
  
  doc_words <- append(doc_words, words_test)
  
  doc_words <- unlist(doc_words, use.names = FALSE)
  
  
  cleaned_words[[i]] <- doc_words
  
  i <- i + 1
  
}

cleaned_words <- unlist(cleaned_words, use.names = FALSE)


correct <- hunspell_check(cleaned_words)

num2 <- cleaned_words[!correct]

num2[1:50]
  
  
  
}

clean_find(high_rate)



```